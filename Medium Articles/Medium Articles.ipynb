{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medium Articles\n",
    "\n",
    "This notebook web scrapes medium articles for key statistics. It can pull data for a specific year, and select publishers. The information obtained will output a dataframe including the columns below.\n",
    "\n",
    "- id -- unique ID number for each article\n",
    "- date -- the date the article was published\n",
    "- url -- link to the articles webpage\n",
    "- title -- the main title for the article (aka. Header 1)\n",
    "- subtitle -- the sub header for the article (aka. Header 2)\n",
    "- author_username -- the authors Meidum username referenced without `@` prefix\n",
    "- claps -- the number of claps received for the article\n",
    "- responses -- the number of responses received for the article\n",
    "- reading_time -- the number of minutes it takes to read the entire article\n",
    "- publication -- the main publication used by the writer for the article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to format days in month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_day(day):\n",
    "    \"\"\" \n",
    "    Formating month by number of days\n",
    "    \"\"\"\n",
    "    \n",
    "    month_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    m = 0\n",
    "    d = 0\n",
    "    while day > 0:\n",
    "        d = day\n",
    "        day -= month_days[m]\n",
    "        m += 1\n",
    "    return (m, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to obtain and convert claps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claps(claps_str):\n",
    "    \"\"\"\n",
    "    Formating the claps when pulled from medium.\n",
    "    Medium record claps abbreviated with a K for thousands\"\"\"\n",
    "    \n",
    "    if (claps_str is None) or (claps_str == '') or (claps_str.split is None):\n",
    "        return 0\n",
    "    split = claps_str.split('K')\n",
    "    claps = float(split[0])\n",
    "    claps = int(claps*1000) if len(split) == 2 else int(claps)\n",
    "    return claps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to scrape article for information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(year, days, urls):\n",
    "    \"\"\"\n",
    "    Function to pull medium artciles for specific urls and dates\n",
    "    \n",
    "    Arguments:\n",
    "    year -- the year you would like to pull articles from.\n",
    "    days -- the number of random days you would like articles pulled from.\n",
    "    urls -- {dictionary} listing the name of the publication and source for article archives.\n",
    "    \n",
    "    \"\"\"\n",
    "    global medium_df\n",
    "    \n",
    "    # function variables\n",
    "    data = []\n",
    "    selected_days = random.sample([i for i in range(1, 366)], days)\n",
    "    article_id = 0\n",
    "    year = year\n",
    "    i = 0\n",
    "    n = len(selected_days)\n",
    "\n",
    "    for d in selected_days:\n",
    "\n",
    "        # formatting each url passing through\n",
    "        i += 1\n",
    "        month, day = convert_day(d)\n",
    "        date = '{0}-{1:02d}-{2:02d}'.format(year, month, day)\n",
    "        print(f'Downloading {i} / {n} : {date}')\n",
    "        for publication, url in urls.items():\n",
    "            response = requests.get(url.format(year, month, day), allow_redirects=True)\n",
    "\n",
    "            if not response.url.startswith(url.format(year, month, day)):\n",
    "                continue\n",
    "\n",
    "            # setting soup variables\n",
    "            page = response.content\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            articles = soup.find_all(\n",
    "                \"div\",\n",
    "                class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\")\n",
    "\n",
    "            # saving variables from articles\n",
    "            for article in articles:\n",
    "                title = article.find(\"h3\", class_=\"graf--title\")\n",
    "                if title is None:\n",
    "                    continue\n",
    "\n",
    "                title = title.contents[0]\n",
    "                article_id += 1\n",
    "                author_box = article.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "                author_url = author_box.find('a')['href'] \n",
    "                author_username = author_url.split('@')[1].strip().lower()\n",
    "                subtitle = article.find(\"h4\", class_=\"graf--subtitle\")\n",
    "                subtitle = subtitle.contents[0] if subtitle is not None else ''\n",
    "                article_url = article.find_all(\"a\")[3]['href'].split('?')[0]\n",
    "                claps = get_claps(article.find_all(\"button\")[1].contents[0])\n",
    "                reading_time = article.find(\"span\", class_=\"readingTime\")\n",
    "                reading_time = 0 if reading_time is None else int(reading_time['title'].split(' ')[0])\n",
    "                responses = article.find_all(\"a\")\n",
    "\n",
    "                if len(responses) == 7:\n",
    "                    responses = responses[6].contents[0].split(' ')\n",
    "                    if len(responses) == 0:\n",
    "                        responses = 0\n",
    "                    else:\n",
    "                        responses = responses[0]\n",
    "                else:\n",
    "                    responses = 0\n",
    "\n",
    "\n",
    "                data.append([article_id, date, article_url, title,\n",
    "                             subtitle, author_username, claps, responses,\n",
    "                             reading_time, publication])\n",
    "                \n",
    "    medium_df = pd.DataFrame(data, columns=[\n",
    "                                            'id', 'date', 'url', 'title', 'subtitle',\n",
    "                                            'author_username', 'claps', 'responses',\n",
    "                                            'reading_time', 'publication']).set_index('id')\n",
    "    \n",
    "    medium_df.to_csv('medium_articles.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs for the Function\n",
    "\n",
    "You can enter a dictionary with a list of the publisher's name and formatted link to obtain the articles in their archive.\n",
    "\n",
    "**Note:** Entering a large number for days will result in a long run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of urls and publications from medium to parse\n",
    "urls = {\n",
    "    'Towards Data Science'    : 'https://towardsdatascience.com/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Analytics Vidhya'        : 'https://medium.com/analytics-vidhya/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Data Insight Action'     : 'https://medium.com/data-insights-action/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'DataSeries'              : 'https://medium.com/dataseries/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Ocean Protocol'          : 'https://blog.oceanprotocol.com/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Hacking Analyics'        : 'https://medium.com/analytics-and-data/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Nightingale'             : 'https://medium.com/nightingale/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'The Startup'             : 'https://medium.com/swlh/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Data Driven Investor'    : 'https://medium.com/datadriveninvestor/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'IBM Data and AI'         : 'https://medium.com/ibm-data-ai/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Data Science Brigade'    : 'https://medium.com/data-science-brigade/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Kaggle Blog'             : 'https://medium.com/kaggle-blog/archive/{0}/{1:02d}/{2:02d}',\n",
    "    'Data & Society: Points'  : 'https://points.datasociety.net/archive/{0}/{1:02d}/{2:02d}'\n",
    "}\n",
    "\n",
    "year = 2020 # year to obtain data\n",
    "days = 300 # number of random days (input should be < 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_article(year, days, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234.653px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
